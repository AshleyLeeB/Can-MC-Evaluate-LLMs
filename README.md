# Can multiple-choice questions really be useful in detecting the abilities of LLMs?

<table align="center" style="border: 1px solid white;">
  <tr>
    <td style="border: 1px solid white;"><img src="figs/meetyou-logo.png" width="250"/></td>
    <td style="border: 1px solid white;"><img src="figs/Ethical-logo.jpg" width="60"/></td>
  </tr>
</table>
Multiple-choice questions (MCQs) are widely used in knowledge ability evaluation of large language models (LLMs) due to their simplicity in form and efficiency during inference. However, concerns have been raised regarding whether MCQs can really reveal the true ability of LLMs; specifically, since LLMs are usually utilized in knowledge-intensive scenarios where the models are required to do long-form generation (LFG), using MCQs for evaluation naturally introduces misalignment between what is tested and what is needed. 
## Background and Experimental Details
## Are LLMs sensitive to the order of candidate answers?
## Multiple Choice Questions vs Long Form Generation Questions

**Authors**:

<div style="overflow: hidden;">
  <ul>
    <li>Wangyue Li  (<a href="mailto:alee90792@gmail.com">alee90792@gmail.com</a>)
    <li>Liangzhi Li (<a href="mailto:liliangzhi@xiaoyouzi.com">liliangzhi@xiaoyouzi.com</a>)
    <li>Tong Xiang  (<a href="mailto:xiangtong@xiaoyouzi.com">xiangtong@xiaoyouzi.com</a>)
    <li>Xiao Liu (<a href="mailto:liuxiao@xiaoyouzi.com">liuxiao@xiaoyouzi.com</a>)
    <li>Wei Deng (<a href="mailto:dengwei@swufe.edu.cn">dengwei@swufe.edu.cn</a>)
    <li>Noa Garcia  (<a href="mailto:noagarcia@ids.osaka-u.ac.jp">noagarcia@ids.osaka-u.ac.jp</a>)<sup>*</sup>
  </ul>
</div>

<sup>*</sup>Corresponding author.
